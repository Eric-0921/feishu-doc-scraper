#!/usr/bin/env python3
import argparse
import asyncio
import json
import sys
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ° path ä»¥ä¾¿å¯¼å…¥ core
sys.path.append(str(Path(__file__).parent))

from core.config import ScraperConfig
from scrapers.feishu_copy import FeishuCopyScraper

async def main():
    parser = argparse.ArgumentParser(description="é£ä¹¦æ–‡æ¡£æŠ“å–å™¨ V5 (æ¶æ„é‡æ„ç‰ˆ)")
    parser.add_argument("--structure", type=str, required=True, help="ç›®å½•ç»“æ„æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--id-range", type=str, default=None, help="æŒ‡å®š ID èŒƒå›´æŠ“å– (æ ¼å¼: start-end, ä¾‹å¦‚: 0-1000)")
    parser.add_argument("--limit", type=int, default=0, help="é™åˆ¶æŠ“å–æ•°é‡ (0=ä¸é™)")
    parser.add_argument("--output-dir", type=str, default=None, help="è‡ªå®šä¹‰è¾“å‡ºç›®å½•å")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–é…ç½®
    config = ScraperConfig()
    
    # è·¯å¾„å¤„ç†
    structure_path = Path(args.structure)
    if not structure_path.exists():
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°ç»“æ„æ–‡ä»¶ {structure_path}")
        sys.exit(1)
        
    # åŠ¨æ€ç¡®å®šè¾“å‡ºç›®å½•å’ŒçŠ¶æ€æ–‡ä»¶å
    task_name = structure_path.stem.replace('_structure', '')
    
    if args.output_dir:
        config.OUTPUT_DIR = Path(args.output_dir)
    else:
        config.OUTPUT_DIR = config.BASE_DIR / "docs" / task_name
    
    state_file = config.LOG_DIR / f"{task_name}_state.json"
    report_file = config.LOG_DIR / f"{task_name}_report.md"
    
    print(f"ğŸš€ å¯åŠ¨ä»»åŠ¡: {task_name}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {config.OUTPUT_DIR}")
    print(f"ğŸ“ çŠ¶æ€æ–‡ä»¶: {state_file}")
    
    # åŠ è½½ä»»åŠ¡åˆ—è¡¨
    with open(structure_path, 'r', encoding='utf-8') as f:
        nodes = json.load(f)
        
    # è¿‡æ»¤æœ‰æ•ˆ URL
    harvest_list = [n for n in nodes if n.get('url') and n['url'].startswith('http')]
    
    # å¤„ç† ID èŒƒå›´è¿‡æ»¤
    if args.id_range:
        try:
            start_id, end_id = map(int, args.id_range.split('-'))
            harvest_list = [n for n in harvest_list if start_id <= n['id'] <= end_id]
            print(f"ğŸ“ èŒƒå›´æ¨¡å¼: å·²ç­›é€‰ ID åœ¨ {start_id} åˆ° {end_id} ä¹‹é—´çš„é¡µé¢")
        except ValueError:
            print(f"âŒ é”™è¯¯: æ— æ•ˆçš„ ID èŒƒå›´æ ¼å¼ '{args.id_range}'ï¼Œè¯·ä½¿ç”¨ start-end æ ¼å¼")
            sys.exit(1)

    if args.limit > 0:
        harvest_list = harvest_list[:args.limit]
        print(f"âš ï¸  æµ‹è¯•æ¨¡å¼é™åˆ¶: ä»…æŠ“å–å‰ {args.limit} é¡µ")
    
    # å¯åŠ¨æŠ“å–å™¨
    scraper = FeishuCopyScraper(config, state_file, report_file)
    await scraper.run(harvest_list)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
