#!/usr/bin/env python3
import argparse
import asyncio
import json
import sys
from pathlib import Path

# æ·»åŠ å½“å‰ç›®å½•åˆ° path ä»¥ä¾¿å¯¼å…¥ core
sys.path.append(str(Path(__file__).parent))

from core.config import ScraperConfig
from scrapers.feishu_copy import FeishuCopyScraper

async def main():
    parser = argparse.ArgumentParser(description="é£ä¹¦æ–‡æ¡£æŠ“å–å™¨ V5 (æ¶æ„é‡æ„ç‰ˆ)")
    parser.add_argument("--structure", type=str, required=True, help="ç›®å½•ç»“æ„æ–‡ä»¶è·¯å¾„ (ä¾‹å¦‚: configs/tutorial_structure.json)")
    parser.add_argument("--limit", type=int, default=0, help="é™åˆ¶æŠ“å–é¡µé¢æ•° (0=å…¨é‡)")
    parser.add_argument("--output-dir", type=str, default=None, help="è‡ªå®šä¹‰è¾“å‡ºç›®å½•å (é»˜è®¤æ ¹æ® structure æ–‡ä»¶åè‡ªåŠ¨ç”Ÿæˆ)")
    
    args = parser.parse_args()
    
    # åˆå§‹åŒ–é…ç½®
    config = ScraperConfig()
    
    # è·¯å¾„å¤„ç†
    structure_path = Path(args.structure)
    if not structure_path.exists():
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°ç»“æ„æ–‡ä»¶ {structure_path}")
        sys.exit(1)
        
    # åŠ¨æ€ç¡®å®šè¾“å‡ºç›®å½•å’ŒçŠ¶æ€æ–‡ä»¶å
    # ä¾‹å¦‚ guide_structure.json -> guide
    task_name = structure_path.stem.replace('_structure', '')
    
    # å¦‚æœæŒ‡å®šäº† output-dirï¼Œä¼˜å…ˆçº§æœ€é«˜ï¼›å¦åˆ™æ ¹æ® task_name è‡ªåŠ¨åœ¨ docs/ ä¸‹åˆ›å»º
    if args.output_dir:
        config.OUTPUT_DIR = Path(args.output_dir)
    else:
        # e.g., docs/tutorial
        config.OUTPUT_DIR = config.BASE_DIR / "docs" / task_name
    
    # çŠ¶æ€æ–‡ä»¶æ”¾åœ¨ logs/ ä¸‹ï¼Œé¿å…æ±¡æŸ“æ ¹ç›®å½•
    state_file = config.LOG_DIR / f"{task_name}_state.json"
    report_file = config.LOG_DIR / f"{task_name}_report.md"
    
    print(f"ğŸš€ å¯åŠ¨ä»»åŠ¡: {task_name}")
    print(f"ğŸ“‚ è¾“å‡ºç›®å½•: {config.OUTPUT_DIR}")
    print(f"ğŸ“ çŠ¶æ€æ–‡ä»¶: {state_file}")
    
    # åŠ è½½ä»»åŠ¡åˆ—è¡¨
    with open(structure_path, 'r', encoding='utf-8') as f:
        nodes = json.load(f)
        
    # è¿‡æ»¤æœ‰æ•ˆ URL
    harvest_list = [n for n in nodes if n.get('url') and n['url'].startswith('http')]
    
    if args.limit > 0:
        harvest_list = harvest_list[:args.limit]
        print(f"âš ï¸  æµ‹è¯•æ¨¡å¼é™åˆ¶: ä»…æŠ“å–å‰ {args.limit} é¡µ")
    
    # å¯åŠ¨æŠ“å–å™¨
    scraper = FeishuCopyScraper(config, state_file, report_file)
    await scraper.run(harvest_list)

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        print("\n\nğŸ›‘ ç”¨æˆ·ä¸­æ–­ç¨‹åº")
