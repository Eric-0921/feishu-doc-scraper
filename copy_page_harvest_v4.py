#!/usr/bin/env python3
"""
é£ä¹¦æ–‡æ¡£å®˜æ–¹ Markdown æŠ“å–å™¨ V4 (äººç±»è¡Œä¸ºæ¨¡æ‹Ÿç‰ˆ)

ç›¸æ¯” V3 çš„æ”¹è¿›ï¼š
- æ›´åŠ éšæœºåŒ–çš„å»¶è¿Ÿæ¨¡å¼ï¼Œæ¨¡æ‹ŸçœŸå®äººç±»è¡Œä¸º
- ä½¿ç”¨é«˜æ–¯åˆ†å¸ƒè€Œéå‡åŒ€åˆ†å¸ƒçš„å»¶è¿Ÿ
- å¶å°”è¾ƒé•¿çš„"ä¼‘æ¯"åœé¡¿
- éšæœºçš„é¼ æ ‡ç§»åŠ¨å’Œæ»šåŠ¨

å»¶è¿Ÿç­–ç•¥ï¼š
- åŸºç¡€å»¶è¿Ÿ: 1.5-3.5ç§’ (é«˜æ–¯åˆ†å¸ƒï¼Œä¸­å¿ƒ2.5ç§’)
- æ¯ 5-15 é¡µ: é¢å¤– 5-15 ç§’"ä¼‘æ¯"åœé¡¿
- éšæœºé¡µé¢æ»šåŠ¨å’Œé¼ æ ‡ç§»åŠ¨

ä½œè€…: AI Assistant
æœ€åæ›´æ–°: 2026-02-03
"""
import os
import sys
import json
import asyncio
import random
import signal
import time
import logging
import math
from pathlib import Path
from urllib.parse import urlparse
from datetime import datetime

from playwright.async_api import async_playwright

try:
    from tqdm import tqdm
except ImportError:
    print("[è­¦å‘Š] tqdm æœªå®‰è£…ï¼Œä½¿ç”¨ç®€åŒ–è¿›åº¦æ˜¾ç¤º")
    tqdm = None


# ============ é…ç½® ============
OUTPUT_DIR = Path("docs")
# é»˜è®¤å€¼ï¼Œå°†åœ¨ main ä¸­æ ¹æ® structure å‚æ•°æ›´æ–°
STATE_FILE = Path("harvest_state.json")
LOG_FILE = Path("harvest.log")
REPORT_FILE = Path("harvest_report.md")
ANTI_BOT_KEYWORDS = ["captcha", "challenge", "human verification", "è¯·å®ŒæˆéªŒè¯"]

# æ›´è‡ªç„¶çš„å»¶è¿Ÿé…ç½®
DELAY_BASE = 2.5      # åŸºç¡€å»¶è¿Ÿä¸­å¿ƒå€¼ (ç§’)
DELAY_SIGMA = 1.0     # é«˜æ–¯åˆ†å¸ƒæ ‡å‡†å·®
DELAY_MIN = 1.0       # æœ€å°å»¶è¿Ÿ
DELAY_MAX = 6.0       # æœ€å¤§å»¶è¿Ÿ
BREAK_INTERVAL = (5, 15)   # æ¯ N é¡µä¼‘æ¯ä¸€æ¬¡çš„èŒƒå›´
BREAK_DURATION = (5, 15)   # ä¼‘æ¯æ—¶é•¿èŒƒå›´ (ç§’)

MAX_RETRIES = 3
MIN_CONTENT_SIZE = 200

# ============ æ—¥å¿—è®¾ç½® ============
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler(sys.stdout)
    ]
)
logger = logging.getLogger(__name__)

# ============ å…¨å±€ä¸­æ–­æ ‡å¿— ============
shutdown_requested = False


def signal_handler(signum, frame):
    global shutdown_requested
    if shutdown_requested:
        logger.warning("å¼ºåˆ¶é€€å‡º...")
        sys.exit(1)
    logger.warning("\næ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œæ­£åœ¨ä¿å­˜è¿›åº¦å¹¶ä¼˜é›…é€€å‡º...")
    logger.warning("å†æ¬¡æŒ‰ Ctrl+C å¼ºåˆ¶é€€å‡º")
    shutdown_requested = True


signal.signal(signal.SIGINT, signal_handler)


def random_delay() -> float:
    """
    ç”Ÿæˆæ¨¡æ‹Ÿäººç±»è¡Œä¸ºçš„éšæœºå»¶è¿Ÿ
    ä½¿ç”¨æˆªæ–­é«˜æ–¯åˆ†å¸ƒ
    """
    delay = random.gauss(DELAY_BASE, DELAY_SIGMA)
    delay = max(DELAY_MIN, min(DELAY_MAX, delay))
    return delay


def should_take_break(page_count: int) -> bool:
    """
    åˆ¤æ–­æ˜¯å¦éœ€è¦ä¼‘æ¯
    æ¯éš”éšæœº 5-15 é¡µä¼‘æ¯ä¸€æ¬¡
    """
    interval = random.randint(*BREAK_INTERVAL)
    return page_count > 0 and page_count % interval == 0


async def simulate_human_behavior(page):
    """
    æ¨¡æ‹Ÿäººç±»è¡Œä¸ºï¼šéšæœºæ»šåŠ¨ã€é¼ æ ‡ç§»åŠ¨
    """
    behaviors = ['scroll', 'mouse', 'wait']
    action = random.choice(behaviors)
    
    if action == 'scroll':
        # éšæœºæ»šåŠ¨é¡µé¢
        scroll_amount = random.randint(-200, 200)
        await page.evaluate(f"window.scrollBy(0, {scroll_amount})")
        await asyncio.sleep(0.2)
    elif action == 'mouse':
        # éšæœºé¼ æ ‡ç§»åŠ¨
        x = random.randint(100, 800)
        y = random.randint(100, 600)
        try:
            await page.mouse.move(x, y)
        except:
            pass
    else:
        # çŸ­æš‚ç­‰å¾…
        await asyncio.sleep(random.uniform(0.1, 0.5))


class HarvestState:
    """æŠ“å–çŠ¶æ€ç®¡ç†å™¨"""
    
    def __init__(self):
        self.completed = set()
        self.failed = set()
        self.skipped = set()
        self.file_sizes = {}
        self.start_time = None
        self.load()
    
    def load(self):
        if STATE_FILE.exists():
            try:
                with open(STATE_FILE, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    self.completed = set(data.get('completed', []))
                    self.failed = set(data.get('failed', []))
                    self.skipped = set(data.get('skipped', []))
                    self.file_sizes = data.get('file_sizes', {})
                logger.info(f"å·²åŠ è½½å†å²è¿›åº¦: {len(self.completed)} å®Œæˆ, {len(self.failed)} å¤±è´¥")
            except Exception as e:
                logger.warning(f"æ— æ³•åŠ è½½çŠ¶æ€æ–‡ä»¶: {e}")
    
    def save(self):
        try:
            with open(STATE_FILE, 'w', encoding='utf-8') as f:
                json.dump({
                    'completed': list(self.completed),
                    'failed': list(self.failed),
                    'skipped': list(self.skipped),
                    'file_sizes': self.file_sizes,
                    'last_updated': datetime.now().isoformat()
                }, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.warning(f"æ— æ³•ä¿å­˜çŠ¶æ€: {e}")
    
    def is_done(self, url):
        return url in self.completed or url in self.skipped
    
    def mark_completed(self, url, file_size=0):
        self.completed.add(url)
        self.file_sizes[url] = file_size
        self.save()
    
    def mark_failed(self, url):
        self.failed.add(url)
        self.save()
    
    def mark_skipped(self, url):
        self.skipped.add(url)
        self.save()


def url_to_folder_path(url: str) -> str:
    try:
        parsed = urlparse(url)
        path_parts = parsed.path.strip('/').split('/')
        if path_parts and path_parts[0] == 'document':
            path_parts = path_parts[1:]
        if len(path_parts) > 1:
            return '/'.join(path_parts[:-1])
        elif len(path_parts) == 1:
            return 'root'
        else:
            return 'uncategorized'
    except:
        return 'uncategorized'


def safe_filename(name: str) -> str:
    illegal_chars = ['/', '\\', ':', '*', '?', '"', '<', '>', '|']
    for char in illegal_chars:
        name = name.replace(char, '_')
    return name[:100]


def detect_anti_bot(page_content: str) -> bool:
    content_lower = page_content.lower()
    for keyword in ANTI_BOT_KEYWORDS:
        if keyword.lower() in content_lower:
            return True
    return False


async def wait_for_user_resume(page):
    print("\n" + "=" * 60)
    print("âš ï¸  æ£€æµ‹åˆ°åè‡ªåŠ¨åŒ–éªŒè¯é¡µé¢ï¼")
    print("è¯·åœ¨æµè§ˆå™¨ä¸­æ‰‹åŠ¨å®ŒæˆéªŒè¯ã€‚")
    print("å®ŒæˆåæŒ‰ Enter ç»§ç»­ï¼Œè¾“å…¥ 'skip' è·³è¿‡ï¼Œè¾“å…¥ 'quit' é€€å‡º")
    print("=" * 60 + "\n")
    
    loop = asyncio.get_event_loop()
    user_input = await loop.run_in_executor(None, input)
    
    if user_input.strip().lower() == 'quit':
        return 'quit'
    elif user_input.strip().lower() == 'skip':
        return 'skip'
    return 'continue'


def generate_report(state: HarvestState, harvest_list: list, elapsed: float):
    small_files = []
    for url, size in state.file_sizes.items():
        if size < MIN_CONTENT_SIZE:
            title = "Unknown"
            for node in harvest_list:
                if node.get('url') == url:
                    title = node.get('title', 'Unknown')
                    break
            small_files.append({'title': title, 'url': url, 'size': size})
    
    small_files.sort(key=lambda x: x['size'])
    
    report = f"""# é£ä¹¦æ–‡æ¡£æŠ“å–æŠ¥å‘Š

**ç”Ÿæˆæ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
**è€—æ—¶**: {elapsed/60:.1f} åˆ†é’Ÿ

## ğŸ“Š ç»Ÿè®¡æ‘˜è¦

| é¡¹ç›® | æ•°é‡ |
|---|---|
| æˆåŠŸ | {len(state.completed)} |
| å¤±è´¥ | {len(state.failed)} |
| è·³è¿‡ | {len(state.skipped)} |

## âš ï¸ æ–‡ä»¶å¤§å°å¼‚å¸¸åˆ—è¡¨ (< {MIN_CONTENT_SIZE} å­—èŠ‚)

ä»¥ä¸‹æ–‡ä»¶å†…å®¹è¿‡å°ï¼Œå¯èƒ½éœ€è¦æ‰‹åŠ¨æ ¸å¯¹ï¼š

| æ ‡é¢˜ | å¤§å° (å­—èŠ‚) | é“¾æ¥ |
|---|---|---|
"""
    
    if small_files:
        for f in small_files:
            report += f"| {f['title']} | {f['size']} | [é“¾æ¥]({f['url']}) |\n"
    else:
        report += "| âœ… æ— å¼‚å¸¸æ–‡ä»¶ | - | - |\n"
    
    if state.failed:
        report += "\n## âŒ æŠ“å–å¤±è´¥åˆ—è¡¨\n\n"
        report += "| é“¾æ¥ |\n|---|\n"
        for url in state.failed:
            title = "Unknown"
            for node in harvest_list:
                if node.get('url') == url:
                    title = node.get('title', 'Unknown')
                    break
            report += f"| [{title}]({url}) |\n"
    
    with open(REPORT_FILE, 'w', encoding='utf-8') as f:
        f.write(report)
    
    logger.info(f"æŠ¥å‘Šå·²ç”Ÿæˆ: {REPORT_FILE}")
    return report


async def copy_page_harvest_v4(limit: int = 0, structure_file: str = "structure.json"):
    """
    äººç±»è¡Œä¸ºæ¨¡æ‹Ÿç‰ˆæŠ“å–ä¸»å‡½æ•°
    
    Args:
        limit: é¡µé¢æ•°é‡é™åˆ¶ (0 = å…¨é‡æŠ“å–)
        structure_file: ç›®å½•ç»“æ„æ–‡ä»¶è·¯å¾„
    """
    global shutdown_requested
    
    state = HarvestState()
    state.start_time = time.time()
    
    structure_path = Path(structure_file)
    if not structure_path.exists():
        logger.error(f"{structure_path} ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œç›®å½•ä¾¦å¯Ÿè„šæœ¬")
        return

    with open(structure_path, 'r', encoding='utf-8') as f:
        nodes = json.load(f)

    harvest_list = [n for n in nodes if n.get('url') and n['url'].startswith('http')]
    pending_list = [n for n in harvest_list if not state.is_done(n.get('url'))]
    
    if limit > 0:
        pending_list = pending_list[:limit]
        logger.info(f"[æ¨¡å¼] é™åˆ¶æŠ“å– {limit} é¡µ")
    
    total = len(harvest_list)
    pending = len(pending_list)
    done = len(state.completed)
    
    print(f"\n{'=' * 50}")
    print(f"ğŸ“Š æŠ“å–ç»Ÿè®¡ (V4 äººç±»æ¨¡æ‹Ÿç‰ˆ)")
    print(f"   æ€»é¡µé¢: {total}")
    print(f"   å·²å®Œæˆ: {done}")
    print(f"   å¾…æŠ“å–: {pending}")
    print(f"{'=' * 50}\n")
    
    if pending == 0:
        logger.info("âœ… æ‰€æœ‰é¡µé¢å·²æŠ“å–å®Œæˆï¼")
        generate_report(state, harvest_list, time.time() - state.start_time)
        return
    
    OUTPUT_DIR.mkdir(exist_ok=True)

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context()
        await context.grant_permissions(['clipboard-read', 'clipboard-write'])
        page = await context.new_page()
        
        success_count = 0
        fail_count = 0
        skip_count = 0
        pages_since_break = 0
        
        if tqdm:
            progress = tqdm(pending_list, desc="æŠ“å–è¿›åº¦", unit="é¡µ")
        else:
            progress = pending_list
        
        for i, node in enumerate(progress):
            if shutdown_requested:
                logger.warning("æ”¶åˆ°ä¸­æ–­ä¿¡å·ï¼Œä¿å­˜è¿›åº¦å¹¶é€€å‡º...")
                break
            
            title = node.get('title', 'Unknown')
            url = node.get('url')
            node_id = node.get('id', i)
            
            if tqdm:
                progress.set_postfix_str(f"å½“å‰: {title[:25]}...")
            else:
                elapsed = time.time() - state.start_time
                eta = (elapsed / (i + 1)) * (pending - i - 1) if i > 0 else 0
                print(f"[{i+1}/{pending}] {title} (ETA: {eta/60:.1f}åˆ†é’Ÿ)")
            
            folder_path = url_to_folder_path(url)
            output_folder = OUTPUT_DIR / folder_path
            output_folder.mkdir(parents=True, exist_ok=True)
            
            safe_title = safe_filename(title)
            output_file = output_folder / f"{node_id:04d}_{safe_title}.md"
            
            if output_file.exists() and output_file.stat().st_size > 100:
                if tqdm:
                    progress.write(f"  â­ï¸  è·³è¿‡: {title} (æ–‡ä»¶å·²å­˜åœ¨)")
                skip_count += 1
                state.mark_skipped(url)
                continue
            
            # å¸¦é‡è¯•çš„æŠ“å–é€»è¾‘
            success = False
            last_error = None
            
            for retry in range(MAX_RETRIES):
                if shutdown_requested:
                    break
                    
                try:
                    await page.goto(url, wait_until="networkidle", timeout=30000)
                    
                    # æ¨¡æ‹Ÿäººç±»è¡Œä¸º
                    await simulate_human_behavior(page)
                    
                    page_content = await page.content()
                    if detect_anti_bot(page_content):
                        action = await wait_for_user_resume(page)
                        if action == 'quit':
                            shutdown_requested = True
                            break
                        elif action == 'skip':
                            state.mark_skipped(url)
                            skip_count += 1
                            success = True
                            break
                    
                    try:
                        await page.wait_for_selector(".doc-content", timeout=10000)
                    except:
                        pass
                    
                    await asyncio.sleep(random.uniform(0.5, 1.5))
                    
                    copy_btn = page.locator('button:has-text("å¤åˆ¶é¡µé¢")')
                    
                    if await copy_btn.count() > 0:
                        await copy_btn.first.click()
                        await asyncio.sleep(random.uniform(0.3, 0.8))
                        
                        clipboard_content = await page.evaluate("navigator.clipboard.readText()")
                        
                        if clipboard_content and len(clipboard_content) > 50:
                            content_to_write = f"# {title}\n\n> Source: {url}\n\n---\n\n{clipboard_content}"
                            with open(output_file, 'w', encoding='utf-8') as f:
                                f.write(content_to_write)
                            
                            file_size = len(content_to_write)
                            if tqdm:
                                if file_size < MIN_CONTENT_SIZE:
                                    progress.write(f"  âš ï¸  {title} ({file_size} å­—èŠ‚ - å†…å®¹è¿‡å°)")
                                else:
                                    progress.write(f"  âœ… {title} ({file_size} å­—èŠ‚)")
                            
                            success_count += 1
                            state.mark_completed(url, file_size)
                            success = True
                            break
                        else:
                            last_error = "å‰ªè´´æ¿å†…å®¹ä¸ºç©º"
                            if retry < MAX_RETRIES - 1:
                                logger.warning(f"  é‡è¯• {retry+1}/{MAX_RETRIES}: {title} - {last_error}")
                                await asyncio.sleep(2)
                    else:
                        await page.locator(".doc-content").click()
                        await page.keyboard.press("Control+a")
                        await page.keyboard.press("Control+c")
                        await asyncio.sleep(0.5)
                        
                        clipboard_content = await page.evaluate("navigator.clipboard.readText()")
                        
                        if clipboard_content and len(clipboard_content) > 50:
                            content_to_write = f"# {title}\n\n> Source: {url}\n\n---\n\n{clipboard_content}"
                            with open(output_file, 'w', encoding='utf-8') as f:
                                f.write(content_to_write)
                            file_size = len(content_to_write)
                            success_count += 1
                            state.mark_completed(url, file_size)
                            success = True
                            break
                        else:
                            last_error = "å¤‡ç”¨æ–¹æ¡ˆä¹Ÿå¤±è´¥"
                            
                except Exception as e:
                    last_error = str(e)[:80]
                    if retry < MAX_RETRIES - 1:
                        logger.warning(f"  é‡è¯• {retry+1}/{MAX_RETRIES}: {title} - {last_error}")
                        await asyncio.sleep(2)
            
            if not success and not shutdown_requested:
                if tqdm:
                    progress.write(f"  âŒ {title} (å¤±è´¥: {last_error})")
                logger.error(f"æŠ“å–å¤±è´¥: {title} - {last_error}")
                fail_count += 1
                state.mark_failed(url)
            
            # æ›´è‡ªç„¶çš„éšæœºå»¶è¿Ÿ
            if not shutdown_requested:
                pages_since_break += 1
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¼‘æ¯
                if should_take_break(pages_since_break):
                    break_duration = random.uniform(*BREAK_DURATION)
                    if tqdm:
                        progress.write(f"  â˜• ä¼‘æ¯ {break_duration:.1f} ç§’...")
                    await asyncio.sleep(break_duration)
                    pages_since_break = 0
                else:
                    # æ­£å¸¸éšæœºå»¶è¿Ÿ
                    delay = random_delay()
                    await asyncio.sleep(delay)
        
        await browser.close()
    
    elapsed = time.time() - state.start_time
    print(f"\n{'=' * 50}")
    print(f"ğŸ“Š æŠ“å–å®ŒæˆæŠ¥å‘Š (V4 äººç±»æ¨¡æ‹Ÿç‰ˆ)")
    print(f"   è€—æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ")
    print(f"   æˆåŠŸ: {success_count}")
    print(f"   å¤±è´¥: {fail_count}")
    print(f"   è·³è¿‡: {skip_count}")
    print(f"   æ€»å®Œæˆ: {len(state.completed)}/{total}")
    print(f"{'=' * 50}")
    
    generate_report(state, harvest_list, elapsed)


if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="é£ä¹¦æ–‡æ¡£æŠ“å–å™¨ V4 (äººç±»è¡Œä¸ºæ¨¡æ‹Ÿç‰ˆ)")
    parser.add_argument("--limit", type=int, default=0, help="é™åˆ¶æŠ“å–é¡µé¢æ•° (0=å…¨é‡)")
    parser.add_argument("--structure", type=str, default="structure.json", help="ç›®å½•ç»“æ„æ–‡ä»¶è·¯å¾„")
    args = parser.parse_args()
    
    # æ ¹æ® structure æ–‡ä»¶ååŠ¨æ€è°ƒæ•´çŠ¶æ€å’ŒæŠ¥å‘Šæ–‡ä»¶å
    prefix = Path(args.structure).stem
    STATE_FILE = Path(f"{prefix}_state.json")
    LOG_FILE = Path(f"{prefix}_harvest.log")
    REPORT_FILE = Path(f"{prefix}_report.md")
    
    # æ›´æ–°å…¨å±€é…ç½®ä¸­çš„ Path å¯¹è±¡
    # æ³¨æ„ï¼šç”±äºæˆ‘ä»¬åœ¨ main ä¸­ä¿®æ”¹äº†å…¨å±€å˜é‡ï¼Œè¿™åœ¨è„šæœ¬æ¨¡å¼ä¸‹æ˜¯å¯ä»¥å·¥ä½œçš„
    # ä½†æ›´å®‰å…¨çš„æ–¹å¼æ˜¯ä¼ å…¥è¿™äº›è·¯å¾„
    
    asyncio.run(copy_page_harvest_v4(limit=args.limit, structure_file=args.structure))
